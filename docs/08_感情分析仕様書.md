# 感情分析仕様書
YouTube分析バッチシステム - 感情分析モジュール

## 1. 概要

### 1.1 使用モデル

本システムは3つのHugging Face公開モデルをアンサンブルで使用：

#### 日本語モデル1: christian-phu/bert-finetuned-japanese-sentiment
- **タイプ**: 3クラス分類（negative / neutral / positive）
- **用途**: 日本語コメントの基本的な感情分析

#### 日本語モデル2: kit-nlp/bert-base-japanese-sentiment-irony
- **タイプ**: 2クラス分類（ポジティブ / ネガティブ）+ 皮肉検出
- **用途**: 日本語の皮肉表現や複雑な感情の検出

#### 多言語モデル: cardiffnlp/twitter-xlm-roberta-base-sentiment
- **タイプ**: 3クラス分類（negative / neutral / positive）
- **用途**: 日本語以外の言語の感情分析
- **特徴**: Twitter特化、198言語対応

### 1.2 分類タスク
- **最終出力**: 3クラス分類
  - `pos` (Positive): ポジティブな感情
  - `neg` (Negative): ネガティブな感情
  - `other` (Neutral/Other): 中立または判定困難
- **アンサンブル方式**: 
  - 日本語コメント: 日本語モデル2つの予測を組み合わせ
  - その他言語: 多言語モデルの予測を使用
  - ルールベース: 200以上のパターンマッチングで精度向上

### 1.3 推論環境
- **ハードウェア**: CPU専用（GPU不要）
- **メモリ使用量**: 約3.5GB以下
- **最適化**: PyTorch推論の最適化を実施（シングルスレッド、勾配計算無効化）

## 2. 推論の最適化

### 2.1 PyTorch設定
```python
import torch

# シングルスレッド設定（CPU効率化）
torch.set_num_threads(1)

# 勾配計算を無効化（推論専用）
with torch.no_grad():
    outputs = model(**inputs)
```

### 2.2 推論パラメータ
| パラメータ | 設定値 | 理由 |
|-----------|--------|------|
| `batch_size` | 1 | メモリ使用量を最小化 |
| `max_length` | 128 | トークン長を制限してメモリ節約 |
| `num_threads` | 1 | CPU効率を最適化 |
| `padding` | True | 可変長テキストに対応 |
| `truncation` | True | 長文を128トークンに切り詰め |

### 2.3 モデルロード戦略
```python
# グローバル変数で管理（起動時に1回のみロード）
_model = None
_tokenizer = None

def load_model() -> None:
    """モデルを1回だけロードする"""
    global _model, _tokenizer
    if _model is not None:
        return  # 既にロード済み
    
    # モデルとトークナイザーをロード
    _tokenizer = AutoTokenizer.from_pretrained(model_path)
    _model = AutoModelForSequenceClassification.from_pretrained(model_path)
    _model.eval()  # 評価モードに設定
```

**重要**: ループ内で `from_pretrained()` を呼ばないこと

## 3. 推論フロー

### 3.1 推論処理の流れ
```
入力テキスト
  ↓
前処理（URL除去、HTML除去、空白正規化）
  ↓
トークン化（max_length=128）
  ↓
PyTorch推論（torch.no_grad()）
  ↓
Softmax（確率計算）
  ↓
Argmax（最高確率のラベルを選択）
  ↓
確率スコア算出（positive/negative/neutral）
  ↓
出力（dict形式）
```

### 3.2 前処理
```python
def _preprocess_text(text: str) -> str:
    # URLを除去
    text = re.sub(r'https?://\S+', '', text)
    
    # HTMLタグを除去
    text = re.sub(r'<[^>]+>', '', text)
    
    # 空白を正規化
    text = ' '.join(text.split())
    
    return text.strip()
```

### 3.3 推論コード例
```python
def _pytorch_inference(text: str) -> dict:
    # トークン化
    inputs = _tokenizer(
        text,
        padding=True,
        truncation=True,
        max_length=128,
        return_tensors='pt'
    )

    # 推論（勾配なし）
    with torch.no_grad():
        outputs = _model(**inputs)
        logits = outputs.logits

        # 確率を計算
        probabilities = torch.softmax(logits, dim=-1)

    # 確率スコアを辞書形式で返す
    return {
        "positive": probabilities[0][2].item(),  # モデル依存
        "negative": probabilities[0][0].item(),
        "neutral": probabilities[0][1].item(),
        "language": "ja"
    }
```

## 4. ラベルマッピング

### 4.1 モデル出力の正規化
異なるモデルが異なるラベル形式を返すため、統一的にマッピング：

| モデル出力 | 正規化後 |
|-----------|---------|
| `pos`, `positive`, `LABEL_1`, `1` | `pos` |
| `neg`, `negative`, `LABEL_0`, `0` | `neg` |
| `other`, `neutral`, `LABEL_2`, `2` | pos/negのスコア比較 |

### 4.2 中立（neutral）の扱い
中立と判定された場合：
1. pos と neg のスコア（softmax確率）を比較
2. 高い方のラベルを採用
3. 同点の場合は `pos` を返す

## 5. フォールバック戦略

### 5.1 ルールベース分類
モデルロード失敗時や推論エラー時に使用：

```python
def _rule_based_classify(text: str) -> str:
    positive_words = ['最高', '素晴らしい', '良い', ...]
    negative_words = ['つまらない', 'ひどい', '悪い', ...]
    
    pos_count = sum(1 for word in positive_words if word in text)
    neg_count = sum(1 for word in negative_words if word in text)
    
    return 'pos' if pos_count >= neg_count else 'neg'
```

### 5.2 エラーハンドリング
- **モデルロード失敗**: ルールベース分類に切り替え
- **推論エラー**: ルールベース分類に切り替え
- **空文字列**: デフォルトで `pos` を返す

## 6. Fine-tuning

### 6.1 概要
独自データでモデルを再学習可能：
- **スクリプト**: `app/train_sentiment.py`
- **データセット**: Hugging Face Datasets から自動取得
- **学習環境**: CPU専用、メモリ3.5GB制限対応

### 6.2 Fine-tunedモデルの使用
環境変数で切り替え：
```bash
USE_FINETUNED_MODEL=true
FINETUNED_MODEL_PATH=./models/sentiment-finetuned
```

詳細は `app/train_sentiment.py` のコード内コメントを参照。

## 7. パフォーマンス

### 7.1 メモリ使用量
- **モデルサイズ**: 約 270MB (DistilBERT)
- **推論時メモリ**: 約 500MB - 1GB
- **合計**: 1.5GB以下（3.5GB制限内）

### 7.2 推論速度
- **CPU**: 約 50-100ms / コメント
- **10コメント**: 約 0.5-1秒
- **100コメント**: 約 5-10秒

### 7.3 精度
- **デフォルトモデル**: 基準なし（事前学習済み）
- **Fine-tunedモデル**: 70-80% accuracy（データセット依存）

## 8. 制約事項

### 8.1 技術的制約
- **GPU不使用**: CPUのみで動作
- **並列処理禁止**: メモリ効率優先で順次処理
- **トークン長制限**: 128トークン（約85-100文字程度）
- **バッチ処理なし**: 1コメントずつ処理

### 8.2 分類の制約
- **3値分類**: positive/negative/neutral の確率スコアを出力
- **日本語特化**: 日本語以外は多言語モデル（XLM-RoBERTa）で対応
- **短文に最適化**: 長文は切り詰められる

## 9. 今後の改善案

### 9.1 精度向上
- より大規模な日本語感情分析データセットでFine-tuning
- データ拡張（Data Augmentation）の導入
- アンサンブル学習の検討

### 9.2 速度向上
- ONNX Runtimeへの変換
- 量子化（Quantization）の導入
- バッチ処理の再検討（メモリ許容範囲内）

### 9.3 機能拡張
- 感情の強度（intensity）検出の追加
- アスペクトベース感情分析への拡張
- リアルタイム分析APIの提供
